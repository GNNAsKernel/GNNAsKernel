train:
  batch_size: 128
  lr_patience: 20
  epochs: 100 #150
  # dropout: 0.3

model:
  mini_layers: 1
  hidden_size: 128
  num_layers: 4
  pool: add
  embs: (0, 1, 2)
  embs_combine_mode: 'concat'

subgraph:
  hops: 3
  # walk_length: 8
  # walk_p: 1.
  # walk_q: 1.
  # walk_repeat: 4